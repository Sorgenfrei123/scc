import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import os
import time
from tqdm import tqdm
import warnings
import math
warnings.filterwarnings('ignore')

# æ··åˆç²¾åº¦è®­ç»ƒ
try:
    from torch.cuda.amp import autocast, GradScaler
    AMP_AVAILABLE = True
except ImportError:
    AMP_AVAILABLE = False
    print("æ··åˆç²¾åº¦è®­ç»ƒä¸å¯ç”¨")

class AmazonReviewDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # æ ‡ç­¾æ˜ å°„
        self.label_map = {'NEG': 0, 'NEU': 1, 'POS': 2}
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # ç¼–ç æ–‡æœ¬
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.label_map[label], dtype=torch.long)
        }

class OptimizedBERTTrainer:
    def __init__(self, model_name='bert-base-chinese', num_classes=3, dropout_rate=0.4):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler() if (self.device.type == 'cuda' and AMP_AVAILABLE) else None
        
        # åŠ è½½tokenizerå’Œæ¨¡å‹ - ç›´æ¥ä½¿ç”¨åŸå§‹BERTæ¨¡å‹ï¼Œé€šè¿‡å‚æ•°è®¾ç½®dropout
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertForSequenceClassification.from_pretrained(
            model_name, 
            num_labels=num_classes,
            hidden_dropout_prob=dropout_rate,  # BERTå†…éƒ¨çš„dropout
            attention_probs_dropout_prob=dropout_rate  # æ³¨æ„åŠ›æœºåˆ¶çš„dropout
        )
        self.model.to(self.device)
        
        # æ ‡ç­¾æ˜ å°„
        self.label_map = {'NEG': 0, 'NEU': 1, 'POS': 2}
        self.reverse_label_map = {v: k for k, v in self.label_map.items()}
        
        print("ä¼˜åŒ–æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"æ··åˆç²¾åº¦è®­ç»ƒ: {'å¯ç”¨' if self.scaler else 'ç¦ç”¨'}")
        print(f"Dropoutç‡: {dropout_rate}")
    
    def calculate_optimal_max_length(self, train_df, text_column='cleaned_text', max_cap=256):
        """æ™ºèƒ½è®¡ç®—æœ€ä¼˜max_length - åŸºäºæ•°æ®åˆ†å¸ƒ"""
        text_lengths = train_df[text_column].str.len()
        
        # è®¡ç®—ç»Ÿè®¡é‡
        quantile_95 = int(text_lengths.quantile(0.95))
        quantile_90 = int(text_lengths.quantile(0.90))
        
        # åŠ¨æ€é€‰æ‹©max_lengthï¼šå¹³è¡¡æ•ˆç‡å’Œè¦†ç›–åº¦
        if quantile_95 <= 128:
            optimal_length = 128
        elif quantile_95 <= 192:
            optimal_length = 192
        else:
            optimal_length = min(quantile_90, max_cap)
        
        print(f"æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {text_lengths.mean():.1f}")
        print(f"  95%åˆ†ä½æ•°: {quantile_95}")
        print(f"  æœ€ç»ˆmax_length: {optimal_length}")
        
        return optimal_length
    
    def prepare_data(self, train_df, val_df, test_df, batch_size=16):
        """å‡†å¤‡æ•°æ®åŠ è½½å™¨"""
        print("å‡†å¤‡æ•°æ®åŠ è½½å™¨...")
        
        # æ™ºèƒ½è®¡ç®—max_length
        max_length = self.calculate_optimal_max_length(train_df)
        
        # è®­ç»ƒé›†
        train_dataset = AmazonReviewDataset(
            texts=train_df['cleaned_text'].values,
            labels=train_df['label'].values,
            tokenizer=self.tokenizer,
            max_length=max_length
        )
        
        # éªŒè¯é›†
        val_dataset = AmazonReviewDataset(
            texts=val_df['cleaned_text'].values,
            labels=val_df['label'].values,
            tokenizer=self.tokenizer,
            max_length=max_length
        )
        
        # æµ‹è¯•é›†æŠ½æ · - å‡å°‘æµ‹è¯•é›†å¤§å°
        if len(test_df) > 2000:
            test_df_sampled = test_df.sample(2000, random_state=42)
            print(f"æµ‹è¯•é›†æŠ½æ ·: {len(test_df_sampled)} æ¡")
        else:
            test_df_sampled = test_df
        
        test_dataset = AmazonReviewDataset(
            texts=test_df_sampled['cleaned_text'].values,
            labels=test_df_sampled['label'].values,
            tokenizer=self.tokenizer,
            max_length=max_length
        )
        
        # æ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        print(f"è®­ç»ƒé›†: {len(train_dataset)} æ¡")
        print(f"éªŒè¯é›†: {len(val_dataset)} æ¡")
        print(f"æµ‹è¯•é›†: {len(test_dataset)} æ¡")
        
        # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
        print(f"\nè®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:")
        print(train_df['label'].value_counts())
        
        return max_length
    
    def get_cosine_schedule_with_warmup(self, optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5):
        """æ”¹è¿›çš„ä½™å¼¦é€€ç«è°ƒåº¦å™¨"""
        from torch.optim.lr_scheduler import LambdaLR
        
        def lr_lambda(current_step):
            if current_step < num_warmup_steps:
                return float(current_step) / float(max(1, num_warmup_steps))
            progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))
        
        return LambdaLR(optimizer, lr_lambda)
    
    def train(self, epochs=20, learning_rate=2e-5, warmup_ratio=0.1):  # ä¿®æ”¹ï¼šepochsä»5æ”¹ä¸º20
        """ä¼˜åŒ–è®­ç»ƒæ¨¡å‹"""
        print("å¼€å§‹ä¼˜åŒ–è®­ç»ƒ...")
        
        # ä¼˜åŒ–å™¨ - ä½¿ç”¨æƒé‡è¡°å‡æ­£åˆ™åŒ–
        no_decay = ['bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in self.model.named_parameters() 
                          if not any(nd in n for nd in no_decay)],
                'weight_decay': 0.01
            },
            {
                'params': [p for n, p in self.model.named_parameters() 
                          if any(nd in n for nd in no_decay)],
                'weight_decay': 0.0
            }
        ]
        optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)
        
        # å­¦ä¹ ç‡è°ƒåº¦ - ä½™å¼¦é€€ç« + warmup
        total_steps = len(self.train_loader) * epochs
        warmup_steps = int(total_steps * warmup_ratio)
        
        scheduler = self.get_cosine_schedule_with_warmup(
            optimizer, 
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
        
        # è®­ç»ƒè®°å½•
        train_losses = []
        train_accuracies = []
        val_accuracies = []
        
        # æ—©åœç­–ç•¥ - ä¿®æ”¹è€å¿ƒå€¼
        best_accuracy = 0
        patience = 5  # ä¿®æ”¹ï¼šä»3æ”¹ä¸º5
        patience_counter = 0
        min_delta = 0.002  # å¢åŠ æœ€å°æ”¹è¿›é˜ˆå€¼
        
        print(f"è®­ç»ƒå‚æ•°:")
        print(f"  æœ€å¤§epochæ•°: {epochs}")
        print(f"  æ—©åœè€å¿ƒ: {patience}")
        print(f"  æœ€å°æ”¹è¿›é˜ˆå€¼: {min_delta}")
        
        for epoch in range(epochs):
            print(f"\nEpoch {epoch + 1}/{epochs}")
            print("-" * 50)
            
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            total_train_loss = 0
            train_correct = 0
            train_total = 0
            
            progress_bar = tqdm(self.train_loader, desc="è®­ç»ƒ")
            for batch_idx, batch in enumerate(progress_bar):
                # ç§»åŠ¨åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # æ··åˆç²¾åº¦è®­ç»ƒ
                if self.scaler:
                    with autocast():
                        outputs = self.model(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            labels=labels
                        )
                        loss = outputs.loss
                        logits = outputs.logits
                    
                    self.scaler.scale(loss).backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                    self.scaler.step(optimizer)
                    self.scaler.update()
                else:
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels
                    )
                    loss = outputs.loss
                    logits = outputs.logits
                    
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                    optimizer.step()
                
                optimizer.zero_grad()
                scheduler.step()
                
                # ç»Ÿè®¡
                total_train_loss += loss.item()
                predictions = torch.argmax(logits, dim=1)
                train_correct += (predictions == labels).sum().item()
                train_total += labels.size(0)
                
                # æ›´æ–°è¿›åº¦æ¡
                current_acc = train_correct / train_total
                current_lr = scheduler.get_last_lr()[0]
                
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{current_acc:.4f}',
                    'LR': f'{current_lr:.2e}'
                })
            
            avg_train_loss = total_train_loss / len(self.train_loader)
            train_accuracy = train_correct / train_total
            train_losses.append(avg_train_loss)
            train_accuracies.append(train_accuracy)
            
            # éªŒè¯é˜¶æ®µ
            val_accuracy, val_report = self.evaluate(self.val_loader)
            val_accuracies.append(val_accuracy)
            
            print(f"\nè®­ç»ƒç»Ÿè®¡:")
            print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            print(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
            print(f"  éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")
            
            # æ”¹è¿›çš„æ—©åœç­–ç•¥
            if val_accuracy > best_accuracy + min_delta:
                best_accuracy = val_accuracy
                patience_counter = 0
                self.save_model('best_optimized_model')
                print(f"  âœ… æ–°çš„æœ€ä½³æ¨¡å‹ä¿å­˜ï¼Œå‡†ç¡®ç‡: {val_accuracy:.4f}")
            else:
                patience_counter += 1
                print(f"  â³ å‡†ç¡®ç‡æœªæå‡ï¼Œè€å¿ƒè®¡æ•°: {patience_counter}/{patience}")
            
            # æ—©åœè§¦å‘
            if patience_counter >= patience:
                print("  ğŸ›‘ æ—©åœè§¦å‘")
                break
        
        print(f"\nè®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.4f}")
        print(f"å®é™…è®­ç»ƒè½®æ•°: {len(train_losses)}")
        return train_losses, train_accuracies, val_accuracies
    
    def evaluate(self, data_loader):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        predictions = []
        true_labels = []
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc="è¯„ä¼°"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                logits = outputs.logits
                batch_predictions = torch.argmax(logits, dim=1)
                
                predictions.extend(batch_predictions.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())
        
        accuracy = accuracy_score(true_labels, predictions)
        
        # è½¬æ¢ä¸ºåŸå§‹æ ‡ç­¾
        pred_labels = [self.reverse_label_map[p] for p in predictions]
        true_labels_str = [self.reverse_label_map[t] for t in true_labels]
        
        report = classification_report(true_labels_str, pred_labels, 
                                     target_names=['å·®è¯„', 'ä¸­è¯„', 'å¥½è¯„'])
        
        return accuracy, report
    
    def predict(self, texts):
        """é¢„æµ‹æ–°æ–‡æœ¬"""
        self.model.eval()
        predictions = []
        probabilities = []
        
        with torch.no_grad():
            for text in texts:
                encoding = self.tokenizer(
                    text,
                    max_length=128,
                    padding='max_length',
                    truncation=True,
                    return_tensors='pt'
                )
                
                input_ids = encoding['input_ids'].to(self.device)
                attention_mask = encoding['attention_mask'].to(self.device)
                
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1)
                
                pred = torch.argmax(logits, dim=1).cpu().item()
                prob = probs.cpu().numpy()[0]
                
                predictions.append(self.reverse_label_map[pred])
                probabilities.append(prob)
        
        return predictions, probabilities
    
    def save_model(self, model_dir):
        """ä¿å­˜æ¨¡å‹"""
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
        
        # ç›´æ¥ä½¿ç”¨transformersçš„ä¿å­˜æ–¹æ³•
        self.model.save_pretrained(model_dir)
        self.tokenizer.save_pretrained(model_dir)
        print(f"æ¨¡å‹ä¿å­˜åˆ°: {model_dir}")
    
    def load_model(self, model_dir):
        """åŠ è½½æ¨¡å‹"""
        self.model = BertForSequenceClassification.from_pretrained(model_dir)
        self.tokenizer = BertTokenizer.from_pretrained(model_dir)
        self.model.to(self.device)
        print(f"æ¨¡å‹ä» {model_dir} åŠ è½½")

def main():
    # ä¼˜åŒ–å‚æ•°è®¾ç½® - å¢åŠ è®­ç»ƒè½®æ¬¡
    BATCH_SIZE = 16
    EPOCHS = 20  # ä¿®æ”¹ï¼šä»5æ”¹ä¸º20
    LEARNING_RATE = 2e-5
    WARMUP_RATIO = 0.1
    DROPOUT_RATE = 0.4  # Dropoutæé«˜åˆ°40%
    
    print("å¼€å§‹æ‰©å±•è®­ç»ƒå•†å“è¯„ä»·åˆ†ç±»æ¨¡å‹...")
    print("=" * 60)
    print("ä½¿ç”¨çš„ä¼˜åŒ–æŠ€æœ¯:")
    print("  âœ… æ··åˆç²¾åº¦è®­ç»ƒ")
    print("  âœ… æ™ºèƒ½max_lengthè®¡ç®—")
    print("  âœ… ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ + Warmup")
    print("  âœ… æ‰©å±•æ—©åœç­–ç•¥(è€å¿ƒ=5)")  # ä¿®æ”¹æè¿°
    print(f"  âœ… é«˜Dropoutæ­£åˆ™åŒ–({DROPOUT_RATE*100}%)")
    print("  âœ… æµ‹è¯•é›†æŠ½æ ·(æœ€å¤š2000æ¡)")
    print("  âœ… æƒé‡è¡°å‡ + æ¢¯åº¦è£å‰ª")
    print(f"  âœ… æ‰©å±•è®­ç»ƒè½®æ¬¡({EPOCHS}è½®)")  # æ–°å¢æè¿°
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    print("åŠ è½½é¢„å¤„ç†æ•°æ®...")
    try:
        train_df = pd.read_csv('data/train_data.csv')
        val_df = pd.read_csv('data/val_data.csv')
        test_df = pd.read_csv('data/test_data.csv')
        
        print(f"è®­ç»ƒé›†: {len(train_df)} æ¡")
        print(f"éªŒè¯é›†: {len(val_df)} æ¡")
        print(f"æµ‹è¯•é›†: {len(test_df)} æ¡")
        
    except FileNotFoundError as e:
        print(f"é”™è¯¯: æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ° - {e}")
        return
    
    # æŠ½æ ·è®­ç»ƒï¼ˆé¿å…æ•°æ®é‡å¤ªå¤§ï¼‰
    if len(train_df) > 30000:  # è¿›ä¸€æ­¥å‡å°‘è®­ç»ƒæ•°æ®é‡
        train_df = train_df.sample(30000, random_state=42)
        print(f"è®­ç»ƒé›†æŠ½æ ·: {len(train_df)} æ¡")
    
    if len(val_df) > 5000:  # å‡å°‘éªŒè¯é›†å¤§å°
        val_df = val_df.sample(5000, random_state=42)
        print(f"éªŒè¯é›†æŠ½æ ·: {len(val_df)} æ¡")
    
    # åˆå§‹åŒ–ä¼˜åŒ–è®­ç»ƒå™¨
    trainer = OptimizedBERTTrainer(dropout_rate=DROPOUT_RATE)
    
    # å‡†å¤‡æ•°æ®ï¼ˆä¼šè‡ªåŠ¨è®¡ç®—æœ€ä¼˜max_lengthï¼‰
    max_length = trainer.prepare_data(train_df, val_df, test_df, batch_size=BATCH_SIZE)
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nå¼€å§‹æ‰©å±•è®­ç»ƒ...")
    print(f"è®­ç»ƒå‚æ•°: batch_size={BATCH_SIZE}, max_length={max_length}")
    print(f"epochs={EPOCHS}, dropout={DROPOUT_RATE}")
    print("=" * 60)
    
    start_time = time.time()
    train_losses, train_accuracies, val_accuracies = trainer.train(
        epochs=EPOCHS,
        learning_rate=LEARNING_RATE,
        warmup_ratio=WARMUP_RATIO
    )
    end_time = time.time()
    
    training_time = (end_time - start_time) / 60
    print(f"\nè®­ç»ƒå®Œæˆï¼Œæ€»è€—æ—¶: {training_time:.2f} åˆ†é’Ÿ")
    print(f"å®é™…è®­ç»ƒè½®æ•°: {len(train_losses)}")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•
    print(f"\nåŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
    trainer.load_model('best_optimized_model')
    
    # æµ‹è¯•é›†è¯„ä¼°
    print(f"\n=== æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼° ===")
    test_accuracy, test_report = trainer.evaluate(trainer.test_loader)
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")
    
    if test_accuracy >= 0.90:
        print("âœ… è¾¾åˆ°ä¼˜ç§€å‡†ç¡®ç‡90%ä»¥ä¸Š!")
    elif test_accuracy >= 0.85:
        print("âœ… è¾¾åˆ°è‰¯å¥½å‡†ç¡®ç‡85%ä»¥ä¸Š!")
    elif test_accuracy >= 0.80:
        print("âš ï¸  å‡†ç¡®ç‡å°šå¯ï¼Œå¯ä»¥æ¥å—")
    else:
        print("âŒ å‡†ç¡®ç‡è¾ƒä½ï¼Œéœ€è¦è°ƒæ•´")
    
    print(f"\næµ‹è¯•é›†åˆ†ç±»æŠ¥å‘Š:")
    print(test_report)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    trainer.save_model('final_optimized_model')
    
    # ç¤ºä¾‹é¢„æµ‹
    print(f"\n=== ç¤ºä¾‹é¢„æµ‹ ===")
    sample_texts = [
        "è¿™ä¸ªå•†å“è´¨é‡å¾ˆå¥½ï¼Œéå¸¸æ»¡æ„ï¼ç‰©æµä¹Ÿå¾ˆå¿«ï¼",
        "ä¸€èˆ¬èˆ¬å§ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„æ„Ÿè§‰ï¼Œä»·æ ¼è¿˜ç®—åˆç†",
        "è´¨é‡å¤ªå·®äº†ï¼Œæ ¹æœ¬ä¸èƒ½ç”¨ï¼Œæµªè´¹é’±"
    ]
    
    predictions, probabilities = trainer.predict(sample_texts)
    for i, text in enumerate(sample_texts):
        pred_label = predictions[i]
        prob = probabilities[i]
        prob_neg, prob_neu, prob_pos = prob[0], prob[1], prob[2]
        
        print(f"æ–‡æœ¬: {text}")
        print(f"é¢„æµ‹: {pred_label}")
        print(f"æ¦‚ç‡: å·®è¯„({prob_neg:.3f}), ä¸­è¯„({prob_neu:.3f}), å¥½è¯„({prob_pos:.3f})")
        print()
    
    print(f"\n=== æ‰©å±•è®­ç»ƒç»“æœæ€»ç»“ ===")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(val_accuracies):.4f}")
    print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
    print(f"è®­ç»ƒè€—æ—¶: {training_time:.2f} åˆ†é’Ÿ")
    print(f"å®é™…è®­ç»ƒè½®æ•°: {len(train_losses)}")
    print(f"Dropoutç‡: {DROPOUT_RATE}")
    print(f"æœ€å¤§è®­ç»ƒè½®æ¬¡: {EPOCHS}")
    print(f"æ—©åœè€å¿ƒå€¼: 5")
    print(f"æ¨¡å‹å·²ä¿å­˜")

if __name__ == "__main__":
    main()