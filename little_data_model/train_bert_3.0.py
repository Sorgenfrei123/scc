import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import os
import time
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class EnhancedAmazonReviewDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.label_map = {'NEG': 0, 'NEU': 1, 'POS': 2}
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # æ›´æ™ºèƒ½çš„ç¼–ç 
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt',
            return_attention_mask=True,
            return_token_type_ids=False  # å¯¹äºåˆ†ç±»ä»»åŠ¡é€šå¸¸ä¸éœ€è¦
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.label_map[label], dtype=torch.long)
        }

class EnhancedBERTTrainer:
    def __init__(self, model_name='bert-base-chinese', num_classes=3):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        
        # å¢å¼ºçš„æ¨¡å‹é…ç½®
        self.model = BertForSequenceClassification.from_pretrained(
            model_name, 
            num_labels=num_classes,
            hidden_dropout_prob=0.2,    # è°ƒæ•´dropout
            attention_probs_dropout_prob=0.1,
            classifier_dropout=0.3,
            output_attentions=False,
            output_hidden_states=False
        )
        self.model.to(self.device)
        
        # æ ‡ç­¾æ˜ å°„
        self.label_map = {'NEG': 0, 'NEU': 1, 'POS': 2}
        self.reverse_label_map = {v: k for k, v in self.label_map.items()}
        
        print("å¢å¼ºBERTæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        
    def calculate_optimal_max_length(self, train_df, text_column='cleaned_text', max_cap=256):
        """æ›´æ™ºèƒ½çš„max_lengthè®¡ç®—"""
        text_lengths = train_df[text_column].astype(str).str.len()
        quantile_90 = int(text_lengths.quantile(0.90))  # ä½¿ç”¨90%åˆ†ä½æ•°
        optimal_length = min(max(quantile_90, 64), max_cap)  # ç¡®ä¿æœ€å°é•¿åº¦
        
        print(f"æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
        print(f"  æœ€å°é•¿åº¦: {text_lengths.min()}")
        print(f"  å¹³å‡é•¿åº¦: {text_lengths.mean():.1f}")
        print(f"  æœ€å¤§é•¿åº¦: {text_lengths.max()}")
        print(f"  90%åˆ†ä½æ•°: {quantile_90}")
        print(f"  æœ€ç»ˆmax_length: {optimal_length}")
        
        return optimal_length
    
    def compute_class_weights(self, labels):
        """è®¡ç®—ç±»åˆ«æƒé‡å¤„ç†ä¸å¹³è¡¡æ•°æ®"""
        class_weights = compute_class_weight(
            'balanced',
            classes=np.array([0, 1, 2]),
            y=labels
        )
        print(f"ç±»åˆ«æƒé‡: {class_weights}")
        return torch.tensor(class_weights, dtype=torch.float).to(self.device)
        
    def prepare_data(self, train_df, val_df, test_df, batch_size=16):
        """å¢å¼ºçš„æ•°æ®å‡†å¤‡"""
        print("å‡†å¤‡æ•°æ®åŠ è½½å™¨...")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        self._check_data_quality(train_df, 'è®­ç»ƒé›†')
        self._check_data_quality(val_df, 'éªŒè¯é›†')
        self._check_data_quality(test_df, 'æµ‹è¯•é›†')
        
        # æ™ºèƒ½è®¡ç®—max_length
        max_length = self.calculate_optimal_max_length(train_df)
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        train_labels = [self.label_map[label] for label in train_df['label'].values]
        self.class_weights = self.compute_class_weights(train_labels)
        
        # æ•°æ®é›†
        train_dataset = EnhancedAmazonReviewDataset(
            texts=train_df['cleaned_text'].values,
            labels=train_df['label'].values,
            tokenizer=self.tokenizer,
            max_length=max_length
        )
        
        val_dataset = EnhancedAmazonReviewDataset(
            texts=val_df['cleaned_text'].values,
            labels=val_df['label'].values,
            tokenizer=self.tokenizer,
            max_length=max_length
        )
        
        test_dataset = EnhancedAmazonReviewDataset(
            texts=test_df['cleaned_text'].values,
            labels=test_df['label'].values,
            tokenizer=self.tokenizer,
            max_length=max_length
        )
        
        # æ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        print(f"è®­ç»ƒé›†: {len(train_dataset)} æ¡")
        print(f"éªŒè¯é›†: {len(val_dataset)} æ¡")
        print(f"æµ‹è¯•é›†: {len(test_dataset)} æ¡")
        
        print(f"\nè®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:")
        label_counts = train_df['label'].value_counts()
        print(label_counts)
        
        return max_length
    
    def _check_data_quality(self, df, dataset_name):
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        print(f"\n{dataset_name}æ•°æ®è´¨é‡æ£€æŸ¥:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
        print(f"  ç©ºå€¼æ•°é‡: {df['cleaned_text'].isnull().sum()}")
        print(f"  æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
        text_lengths = df['cleaned_text'].astype(str).str.len()
        print(f"    æœ€çŸ­: {text_lengths.min()}, æœ€é•¿: {text_lengths.max()}, å¹³å‡: {text_lengths.mean():.1f}")
        
        # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
        label_dist = df['label'].value_counts()
        print(f"  æ ‡ç­¾åˆ†å¸ƒ: {dict(label_dist)}")
        
        # æ£€æŸ¥é‡å¤æ ·æœ¬
        duplicates = df.duplicated(subset=['cleaned_text']).sum()
        print(f"  é‡å¤æ ·æœ¬: {duplicates}")
    
    def train(self, epochs=8, learning_rate=2e-5):
        """å¢å¼ºçš„è®­ç»ƒè¿‡ç¨‹"""
        print("å¼€å§‹å¢å¼ºè®­ç»ƒ...")
        
        # ä¼˜åŒ–å™¨é…ç½®
        no_decay = ['bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in self.model.named_parameters() 
                          if not any(nd in n for nd in no_decay)],
                'weight_decay': 0.01
            },
            {
                'params': [p for n, p in self.model.named_parameters() 
                          if any(nd in n for nd in no_decay)],
                'weight_decay': 0.0
            }
        ]
        
        optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        total_steps = len(self.train_loader) * epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=int(0.1 * total_steps),  # 10%çš„warmup
            num_training_steps=total_steps
        )
        
        # è®­ç»ƒè®°å½•
        train_losses = []
        train_accuracies = []
        val_accuracies = []
        
        best_accuracy = 0
        patience = 3
        patience_counter = 0
        
        print(f"è®­ç»ƒå‚æ•°:")
        print(f"  æœ€å¤§epochæ•°: {epochs}")
        print(f"  å­¦ä¹ ç‡: {learning_rate}")
        print(f"  æ€»æ­¥æ•°: {total_steps}")
        print(f"  Warmupæ­¥æ•°: {int(0.1 * total_steps)}")
        
        for epoch in range(epochs):
            print(f"\nEpoch {epoch + 1}/{epochs}")
            print("-" * 50)
            
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            total_train_loss = 0
            train_correct = 0
            train_total = 0
            
            progress_bar = tqdm(self.train_loader, desc="è®­ç»ƒ")
            for batch_idx, batch in enumerate(progress_bar):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                loss = outputs.loss
                
                # åº”ç”¨ç±»åˆ«æƒé‡
                logits = outputs.logits
                if self.class_weights is not None:
                    loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)
                    loss = loss_fct(logits.view(-1, 3), labels.view(-1))
                
                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                
                # ç»Ÿè®¡
                total_train_loss += loss.item()
                predictions = torch.argmax(logits, dim=1)
                train_correct += (predictions == labels).sum().item()
                train_total += labels.size(0)
                
                current_acc = train_correct / train_total
                current_lr = scheduler.get_last_lr()[0]
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{current_acc:.4f}',
                    'LR': f'{current_lr:.2e}'
                })
            
            avg_train_loss = total_train_loss / len(self.train_loader)
            train_accuracy = train_correct / train_total
            train_losses.append(avg_train_loss)
            train_accuracies.append(train_accuracy)
            
            # éªŒè¯é˜¶æ®µ
            val_accuracy, val_report, val_cm = self.evaluate(self.val_loader)
            val_accuracies.append(val_accuracy)
            
            print(f"\nè®­ç»ƒç»Ÿè®¡:")
            print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            print(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
            print(f"  éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")
            print(f"  è¿‡æ‹Ÿåˆå·®è·: {train_accuracy - val_accuracy:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_accuracy > best_accuracy:
                best_accuracy = val_accuracy
                patience_counter = 0
                self.save_model('best_enhanced_model')
                print(f"  âœ… æ–°çš„æœ€ä½³æ¨¡å‹ä¿å­˜ï¼Œå‡†ç¡®ç‡: {val_accuracy:.4f}")
                
                # ä¿å­˜æ¯ä¸ªç±»åˆ«çš„æœ€ä½³å‡†ç¡®ç‡
                val_details = self.get_detailed_metrics(self.val_loader)
                print(f"  å„ç±»åˆ«å‡†ç¡®ç‡ - å·®è¯„: {val_details['NEG']:.4f}, ä¸­è¯„: {val_details['NEU']:.4f}, å¥½è¯„: {val_details['POS']:.4f}")
            else:
                patience_counter += 1
                print(f"  â³ å‡†ç¡®ç‡æœªæå‡ï¼Œè€å¿ƒè®¡æ•°: {patience_counter}/{patience}")
            
            # æ—©åœ
            if patience_counter >= patience:
                print("  ğŸ›‘ æ—©åœè§¦å‘")
                break
        
        print(f"\nè®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.4f}")
        return train_losses, train_accuracies, val_accuracies
    
    def get_detailed_metrics(self, data_loader):
        """è·å–è¯¦ç»†çš„å„ç±»åˆ«å‡†ç¡®ç‡"""
        self.model.eval()
        predictions = []
        true_labels = []
        
        with torch.no_grad():
            for batch in data_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                batch_predictions = torch.argmax(logits, dim=1)
                
                predictions.extend(batch_predictions.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())
        
        true_labels = np.array(true_labels)
        predictions = np.array(predictions)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        class_accuracies = {}
        for class_id, class_name in self.reverse_label_map.items():
            mask = true_labels == class_id
            if mask.sum() > 0:
                class_acc = (predictions[mask] == class_id).mean()
                class_accuracies[class_name] = class_acc
            else:
                class_accuracies[class_name] = 0.0
                
        return class_accuracies
    
    def evaluate(self, data_loader, dataset_name=""):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        predictions = []
        true_labels = []
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc=f"è¯„ä¼°{dataset_name}"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                batch_predictions = torch.argmax(logits, dim=1)
                
                predictions.extend(batch_predictions.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())
        
        accuracy = accuracy_score(true_labels, predictions)
        
        pred_labels = [self.reverse_label_map[p] for p in predictions]
        true_labels_str = [self.reverse_label_map[t] for t in true_labels]
        
        report = classification_report(true_labels_str, pred_labels, 
                                     target_names=['å·®è¯„', 'ä¸­è¯„', 'å¥½è¯„'])
        
        cm = confusion_matrix(true_labels_str, pred_labels, labels=['NEG', 'NEU', 'POS'])
        
        return accuracy, report, cm
    
    def save_model(self, model_dir):
        """ä¿å­˜æ¨¡å‹"""
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'label_map': self.label_map,
                'reverse_label_map': self.reverse_label_map
            }
        }, f'{model_dir}/model_weights.pth')
        
        self.tokenizer.save_pretrained(model_dir)
        print(f"æ¨¡å‹ä¿å­˜åˆ°: {model_dir}")
    
    def load_model(self, model_dir):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(f'{model_dir}/model_weights.pth', map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        
        if 'model_config' in checkpoint:
            self.label_map = checkpoint['model_config']['label_map']
            self.reverse_label_map = checkpoint['model_config']['reverse_label_map']
        
        print(f"æ¨¡å‹ä» {model_dir} åŠ è½½")

def data_preprocessing_analysis(train_df, val_df, test_df):
    """æ•°æ®é¢„å¤„ç†åˆ†æå’Œå»ºè®®"""
    print("=== æ•°æ®é¢„å¤„ç†åˆ†æ ===")
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    print("\n1. æ ‡ç­¾åˆ†å¸ƒåˆ†æ:")
    for name, df in [('è®­ç»ƒé›†', train_df), ('éªŒè¯é›†', val_df), ('æµ‹è¯•é›†', test_df)]:
        print(f"{name}: {df['label'].value_counts().to_dict()}")
    
    # æ£€æŸ¥æ–‡æœ¬è´¨é‡
    print("\n2. æ–‡æœ¬è´¨é‡åˆ†æ:")
    for name, df in [('è®­ç»ƒé›†', train_df), ('éªŒè¯é›†', val_df), ('æµ‹è¯•é›†', test_df)]:
        text_lengths = df['cleaned_text'].astype(str).str.len()
        print(f"{name} - å¹³å‡é•¿åº¦: {text_lengths.mean():.1f}, ç©ºæ–‡æœ¬: {df['cleaned_text'].isnull().sum()}")
    
    # å»ºè®®
    print("\n3. æ”¹è¿›å»ºè®®:")
    print("   - ç¡®ä¿è®­ç»ƒé›†è¶³å¤Ÿå¤§(å»ºè®®10,000+æ ·æœ¬)")
    print("   - æ£€æŸ¥æ ‡ç­¾æ ‡æ³¨è´¨é‡")
    print("   - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡")
    print("   - ç¡®ä¿æ–‡æœ¬æ¸…æ´—è´¨é‡")

def main():
    # ä¼˜åŒ–å‚æ•°è®¾ç½®
    BATCH_SIZE = 16
    EPOCHS = 8
    LEARNING_RATE = 2e-5
    DATA_DIR = 'data_emoji'
    
    print("å¼€å§‹å¢å¼ºè®­ç»ƒå•†å“è¯„ä»·åˆ†ç±»æ¨¡å‹...")
    print("=" * 60)
    print("æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥:")
    print("  âœ… æ•°æ®è´¨é‡æ£€æŸ¥å’Œæ¸…æ´—")
    print("  âœ… ç±»åˆ«æƒé‡å¤„ç†ä¸å¹³è¡¡")
    print("  âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨")
    print("  âœ… æ›´åˆé€‚çš„max_lengthè®¡ç®—")
    print("  âœ… è¯¦ç»†çš„å„ç±»åˆ«å‡†ç¡®ç‡ç›‘æ§")
    print("=" * 60)
    
    # åŠ è½½é¢„å¤„ç†æ•°æ®
    print("åŠ è½½é¢„å¤„ç†æ•°æ®...")
    try:
        train_df = pd.read_csv(f'{DATA_DIR}/train_data.csv')
        val_df = pd.read_csv(f'{DATA_DIR}/val_data.csv')
        test_df = pd.read_csv(f'{DATA_DIR}/test_data.csv')
        
        print(f"è®­ç»ƒé›†: {len(train_df)} æ¡")
        print(f"éªŒè¯é›†: {len(val_df)} æ¡")
        print(f"æµ‹è¯•é›†: {len(test_df)} æ¡")
        
    except FileNotFoundError as e:
        print(f"é”™è¯¯: æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ° - {e}")
        return
    
    # æ•°æ®é¢„å¤„ç†åˆ†æ
    data_preprocessing_analysis(train_df, val_df, test_df)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = EnhancedBERTTrainer()
    
    # å‡†å¤‡æ•°æ®
    max_length = trainer.prepare_data(train_df, val_df, test_df, batch_size=BATCH_SIZE)
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nå¼€å§‹è®­ç»ƒ...")
    print(f"è®­ç»ƒå‚æ•°: batch_size={BATCH_SIZE}, max_length={max_length}")
    print(f"epochs={EPOCHS}, lr={LEARNING_RATE}")
    print("=" * 60)
    
    start_time = time.time()
    train_losses, train_accuracies, val_accuracies = trainer.train(
        epochs=EPOCHS,
        learning_rate=LEARNING_RATE
    )
    end_time = time.time()
    
    training_time = (end_time - start_time) / 60
    print(f"\nè®­ç»ƒå®Œæˆï¼Œæ€»è€—æ—¶: {training_time:.2f} åˆ†é’Ÿ")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•
    print(f"\nåŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
    trainer.load_model('best_enhanced_model')
    
    # æµ‹è¯•é›†è¯„ä¼°
    print(f"\n=== æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼° ===")
    test_accuracy, test_report, test_cm = trainer.evaluate(trainer.test_loader, "æµ‹è¯•é›†")
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")
    
    # è¯¦ç»†å„ç±»åˆ«å‡†ç¡®ç‡
    test_details = trainer.get_detailed_metrics(trainer.test_loader)
    print(f"\nå„ç±»åˆ«æµ‹è¯•å‡†ç¡®ç‡:")
    for class_name, acc in test_details.items():
        print(f"  {class_name}: {acc:.4f}")
    
    print(f"\næµ‹è¯•é›†åˆ†ç±»æŠ¥å‘Š:")
    print(test_report)
    
    # æ˜¾ç¤ºæ··æ·†çŸ©é˜µ
    print(f"\næµ‹è¯•é›†æ··æ·†çŸ©é˜µ:")
    print("       é¢„æµ‹: å·®è¯„  ä¸­è¯„  å¥½è¯„")
    print("çœŸå®:")
    labels = ['å·®è¯„', 'ä¸­è¯„', 'å¥½è¯„']
    for i, true_label in enumerate(labels):
        print(f"{true_label}:   {test_cm[i][0]:5d} {test_cm[i][1]:5d} {test_cm[i][2]:5d}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history_df = pd.DataFrame({
        'epoch': range(1, len(train_losses) + 1),
        'train_loss': train_losses,
        'train_accuracy': train_accuracies,
        'val_accuracy': val_accuracies
    })
    history_df.to_csv('enhanced_training_history.csv', index=False)
    
    print(f"\nè®­ç»ƒå®Œæˆ!")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
    print(f"è®­ç»ƒè€—æ—¶: {training_time:.2f} åˆ†é’Ÿ")

if __name__ == "__main__":
    main()